# Learning from Democratic Candidates’ Tweets

Using cosine similarities, structured topic models and wordfish to map out the political position of candidates and key issues in advance of the 2020 Democratic Primary Election Debates. Analysis was conducted in May 2018. 

## Overview  

Over the 2020 election cycle, this year's Democratic presidential primary debates are expected to accommmodate a ["historically large primary field"](https://democrats.org/press/dnc-announces-details-for-the-first-two-presidential-primary-debates/). With only 20 available spots and at least 21 individuals who have announced their candidacy, polling numbers and data on donations will be crucial in determining who attends each subsequent debate. Therefore, for those who have not met the minimum poll and fundraising requirements, which comprise of [reporting more than 65,000 unique donors or registering at least 1% in three separate polls by pre-approved organizations](https://www.nytimes.com/2019/05/02/us/politics/democratic-debates-candidates.html), outreach is crucial for them to stay in the race . While email outreach dominates the call for more unique donors, Twitter also plays a vital role in determining what the issues of priority are throughout the election cycle. Ultimately, key topics that gain traction with the electorate will factor into who eventually goes on to be the official democratic nominee. 

**Using Twitter data from accounts of democratic candidates who are qualified to participate in the first Democratic presidential primary debate in June, I aim to explore which issues are being talked about online and map out where candidates and topics stand relative to one another with regards to political position**. Since the official list of candidates is not set as of now, I highlight candidates who meet the fundraising requirements in this analysis because they are most likely to take the stage by the time participants are selected for next month's debates. This is because, in the event that the number of candidates who meet the polling requirements exceeds 20 individuals, some of the other candidates in this analysis may not participate in the upcoming debate. Due to this uncertainty, this analysis does not aim to make predictions or assumptions about how the 2020 Democratic Primary election cycle will pan out.

## Data

To answer the research question highlighted above, this dataset comprises of tweets from 17 candidates, which include 8 who have met both the fundraising and polling requirements and 9 who have only met the polling requirements. After subsetting the data to contain tweets from December 1, 2018 (right after the midterm elections) to May 1, 2018, the dataset used for analysis contains 4,031 tweets.  


## Methodology 

To explore the extent to which we observe common elements between candidates, which inform our understanding of ideological distance, I construct document-feature-matrices to explore pairwise **cosine similarities** between all candidates. To adjust for potential biases that arise as a result of word frequencies, I use `tf-idf` transformations to observe whether perceived similarities based on word counts hold with weighted estimates. To investigate the topics that arise from the candidates' tweets, I use **structured topic models** to identify the issues that candidates are highlighting on social media, as well as their prevalence by candidate. In addition to using the `stm` package to label topics by words with the highest frequency, I also employ keywords-in-context (KWIC) searches to determine what each topic encompasses in the context of this dataset. 

After looking at ideological distance between candidates and topic prevalence, I use a text analysis model called **wordfish** to measure the ideological position of candidates and words used in the text on a one-dimensional scale. This model assumes that one's political position is determined by how frequently they use certain words, and that the frequency at which words are chosen follow a Poisson distribution. Other key assumptions are that a word's occurrence in a text is independent of other words, and that underlying positions in text are assumed to be on a "left-right politics dimension" (Slapin and Proksch, 2008, Grimmer and Stewart 2013). This model is particularly powerful in analyzing political positions of democratics, especially with current distinctions between establishment democrats and progressives. 

## Summary of Results 

Using Twitter data from politicians who announced their candidacy for the 2020 election, I estimated cosine similarities between texts from each candidate and built a structured topic model to gain a better understanding of what politicians are talking about online. Using the findings from cosine similarities and the 9-topic model, I estimated a wordfish model that measures political position and the impact of individual words on partisanship. I found similar patterns between the cosine similarities and the candidate-level findings generated by the wordfish algorithm. While there is not enough information to make any claims about the relationship between cosine similarity and political position in the respective models, looking into the connection between the two could establish a connection that could allow us to gain more insight on how we can identify ideological distance between tweets as consumers of social media. 

Results that I found to be particularly surprising were the estimates for the theta coefficients in the wordfish model, as I thought some of the candidates who were classified as right-of-center relative to other candidates would be farther left, and vice versa. In considering why this may be the case, I speculate that some contributing factors could include the content of the tweets themselves (i.e. promoting campaigns vs. advocating for social justice causes), or that the dataset was not particularly balanced. Accordingly, a follow-up research question I would be interested in further investigating is the extent to which a politician's tweets reflect their individual views and past actions while in office.

Given the strict assumptions of using the estimates and models above, these results should be regarded cautiously. Some of the limitations that I aim to address in future research using social media data includes working with an unbalanced set- in the data collected, the frequency of tweets was varied, with "high momentum" candidates tweeting almost twice as much as their counterparts on average. This likely biased the estimates, and I would expect that some of the results would not be as pronounced if I apply a random sampling strategy that takes these individual-level differences into account. Further, this model could be improved by including other exogenous variables into account, such as embededness in certain social networks and who manages the candidate's social media. 

## Ethics Statement

The data used for this project was collected taking best practices in ethical social media research into account. As collecting social media does not require the approval of an Institutional Review Board (IRB), researchers must cautiously exercise discretion to make sure users' privacy and safety are protected. This is especially the case with Twitter data, as [Twitter's Developer Policies](https://developer.twitter.com/en/developer-terms/policy) are vaguely worded and difficult for the platform to regulate. The main ethical considerations with using Twitter data for research, as outlined by Williams et al. (2017), are as follows: 

- **Consider what type of account tweets are being scraped from.** In the case of this paper, tweets were only scraped from politicians' accounts, which are classified as "public figure accounts" and thus free for anyone to use for research purposes. For researchers looking to scrape tweets from organizational or private individual accounts, additional steps need to take place in order to ensure ethical best practices. 
- **Seek opt-in consent from individuals who tweet sensitive content or can be identified as "vulnerable"**. Even if tweets are anonymized, syntax and other qualities of text can uncover the identity of who wrote the text in the first place. This could be dangerous if certain patterns identified by research lead to vulnerable individuals being targeted. 

While politicians' and other public figure accounts are open for fair use, it is still important to consider whether research will lead to unintended consequences that could harm others. Further, public figures on social media engage with organizational and individual accounts, thus rendering the considerations above necessary before proceeding with research if the involvement of other accounts is sufficiently close. This is why I focused my research on tweets from the politicians themselves, without involving replies or in-depth information about their followers.

## References 

Grimmer, J. and Stewart, B.M., 2013. Text as data: The promise and pitfalls of automatic content analysis methods for political texts. Political analysis, 21(3), pp.267-297.

Slapin, Jonathan B. and Sven-Oliver Proksch. 2008. “A scaling model for estimating time-series party
positions from texts.” American Journal of Political Science 52(3):705–722.

Williams, M.L., Burnap, P. and Sloan, L., 2017. Towards an ethical framework for publishing Twitter data in social research: Taking into account users’ views, online context and algorithmic estimation. Sociology, 51(6), pp.1149-1168.
